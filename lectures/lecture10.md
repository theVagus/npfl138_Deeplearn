### Lecture: 10. Transformer, BERT, ViT
#### Date: Apr 29
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl138/2425/slides/?10
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl138/2425/slides.pdf/npfl138-2425-10.pdf, PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl138/2425/npfl138-2425-10-czech.mp4, CZ Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl138/2425/npfl138-2425-10-czech.practicals.mp4, CZ Practicals
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl138/2425/npfl138-2425-10-english.mp4, EN Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl138/2425/npfl138-2425-10-english.practicals.mp4, EN Practicals
#### Questions: #lecture_10_questions
#### Lecture assignment: tagger_transformer
#### Lecture assignment: sentiment_analysis
#### Lecture assignment: reading_comprehension

- Transformer architecture [[Attention Is All You Need](https://arxiv.org/abs/1706.03762)]
- BERT [[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)]
- RoBERTa [[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)]
- ViT [[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)]
- _MAE [[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)]_
- _DETR [[End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)]_
